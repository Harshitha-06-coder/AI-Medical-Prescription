# -*- coding: utf-8 -*-
"""medical3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QevkXwxgBHMwA1GAiwl79VnA9m_JfEEc
"""

# ================================================
# AI Medical Prescription Verification App (FINAL)
# IBM Granite 3.3 2B Instruct + Gradio
# JSON-SAFE, ERROR-PROOF VERSION
# ================================================

!pip install transformers accelerate gradio -q

import gradio as gr
import json, re
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# =====================================================
# LOAD IBM GRANITE 3.3 2B INSTRUCT
# =====================================================

model_id = "ibm-granite/granite-3.3-2b-instruct"

print("‚öô Loading IBM Granite 3.3 2B‚Ä¶")

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype="auto",
    trust_remote_code=True
)

llm = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    temperature=0.0
)

print("‚úÖ Granite Loaded Successfully")


# =====================================================
# JSON SANITIZER
# =====================================================

def sanitize_json(text):
    """
    Extracts and fixes JSON even if the model outputs broken JSON.
    """

    # Try extracting the last {...} block
    try:
        json_blocks = re.findall(r"\{[\s\S]*?\}", text)
        if not json_blocks:
            return None
        candidate = json_blocks[-1]

        # Fix common JSON problems
        candidate = candidate.replace("\n", " ")
        candidate = re.sub(r",\s*}", "}", candidate)
        candidate = re.sub(r",\s*]", "]", candidate)

        return json.loads(candidate)

    except:
        return None


# =====================================================
# MAIN VERIFICATION FUNCTION
# =====================================================

def verify_prescription(prescription_text):

    prompt = f"""
You are an AI Medical Prescription Verification System.
Return ONLY valid JSON. No explanation. No comments. No extra text.

Required format:
{{
  "drug_validation": {{}},
  "safety_checks": {{}},
  "completeness_check": {{}}
}}

Analyze this prescription and output ONLY the JSON:
"{prescription_text}"
"""

    ai_output = llm(prompt)[0]["generated_text"]

    # Try sanitizing JSON
    json_result = sanitize_json(ai_output)

    # If still invalid ‚Üí return safe fallback
    if json_result is None:
        return {
            "error": "Model did not return valid JSON",
            "raw_model_output": ai_output
        }

    return json_result


# =====================================================
# GRADIO UI
# =====================================================

ui = gr.Interface(
    fn=verify_prescription,
    inputs=gr.Textbox(lines=10, label="Enter Prescription Text"),
    outputs=gr.JSON(label="AI Verification Result"),
    title="üß† AI Medical Prescription Verification App",
    description="Granite 3.3 2B Instruct ‚Äî Fully JSON-Safe Version",
)

ui.launch(share=True)

!pip install --quiet transformers accelerate gradio sentencepiece safetensors evaluate
!pip install --quiet pillow pytesseract regex
!apt-get update -qq && apt-get install -y -qq¬†tesseract-ocr

import torch
print("CUDA available:", torch.cuda.is_available())
print("GPU:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")

# =========================
# AI Prescription Verifier
# Single-file, Colab-friendly
# =========================

# ---------------------------
# 1) (If using Colab) - Install once
# Paste and run in a code cell before running the Python block below:
# ---------------------------
# !pip install --quiet transformers accelerate gradio sentencepiece safetensors evaluate
# !pip install --quiet pillow pytesseract regex
# !apt-get update -qq && apt-get install -y -qq tesseract-ocr

# ---------------------------
# 2) Full script starts here
# ---------------------------
import re
import json
import logging
from typing import Optional, Tuple
from PIL import Image
import pytesseract
import gradio as gr

logging.basicConfig(level=logging.INFO)

# ---------------------------
# Config & sample DB (extendable)
# ---------------------------
MODEL_ID = "ibm-granite/granite-3.3-2b-instruct"  # replace if you want another model
MAX_NEW_TOKENS = 512

DRUG_DB = {
    "paracetamol": {"generic": "paracetamol", "brand": ["crocin", "calpol"], "min": 325, "max": 1000},
    "amoxicillin": {"generic": "amoxicillin", "brand": ["amoxil"], "min": 250, "max": 2000},
    "metformin": {"generic": "metformin", "brand": ["glucophage"], "min": 500, "max": 2000},
    "ibuprofen": {"generic": "ibuprofen", "brand": ["brufen", "advil"], "min": 200, "max": 800},
}

INTERACTIONS = {
    tuple(sorted(("warfarin", "ibuprofen"))): "High risk of bleeding (NSAID + anticoagulant).",
    tuple(sorted(("metformin","contrast"))): "Moderate risk: contrast dyes may increase risk of lactic acidosis.",
}

# ---------------------------
# Utilities: OCR, normalize, matching, dose extraction
# ---------------------------
def ocr_extract(img):
    try:
        if isinstance(img, Image.Image):
            pil = img
        else:
            pil = Image.fromarray(img)
        return pytesseract.image_to_string(pil, lang='eng')
    except Exception as e:
        logging.exception("OCR failed: %s", e)
        return ""

def normalize_name(s: str) -> str:
    return re.sub(r'[^a-z0-9\s]', '', (s or "").lower()).strip()

def match_drug(name: str):
    if not name:
        return None
    n = normalize_name(name)
    if n in DRUG_DB:
        return DRUG_DB[n]
    for k, v in DRUG_DB.items():
        if n in [b.lower() for b in v.get("brand", [])]:
            return v
    # substring heuristic
    for k, v in DRUG_DB.items():
        if k in n or any(b in n for b in v.get("brand", [])):
            return v
    return None

def extract_dose(text: str) -> Optional[float]:
    if not text:
        return None
    txt = text.lower().replace(',', '.')
    m = re.search(r'(\d+(?:\.\d+)?)\s*(mg|g|mcg|¬µg|ug)\b', txt)
    if not m:
        return None
    value = float(m.group(1))
    unit = m.group(2)
    if unit == 'g':
        return value * 1000.0
    if unit in ('mcg','¬µg','ug'):
        return value / 1000.0
    return value  # mg

# ---------------------------
# Robust JSON extraction helper
# ---------------------------
def extract_json_from_text(text: str, start_marker: str="<<<JSON>>>", end_marker: str="<<<ENDJSON>>>") -> Tuple[Optional[dict], str]:
    """
    Try to robustly extract JSON object returned by LLM.
    Returns (parsed_obj_or_None, candidate_text_or_full_text).
    """
    if not text:
        return None, ""

    # 1) marker-based
    if start_marker in text and end_marker in text:
        try:
            start = text.index(start_marker) + len(start_marker)
            end = text.index(end_marker, start)
            candidate = text[start:end].strip()
            parsed = json.loads(candidate)
            return parsed, candidate
        except Exception as e:
            logging.debug("Marker-based parsing failed: %s", e)

    # 2) first {...} block
    m = re.search(r'\{.*\}', text, flags=re.DOTALL)
    if m:
        candidate = m.group(0)
        try:
            parsed = json.loads(candidate)
            return parsed, candidate
        except Exception as e:
            logging.debug("Direct json.loads failed: %s", e)
            # try trimming to balanced braces
            open_idx = candidate.find('{')
            balance = 0
            for i, ch in enumerate(candidate[open_idx:], start=open_idx):
                if ch == '{':
                    balance += 1
                elif ch == '}':
                    balance -= 1
                if balance == 0:
                    try:
                        sub = candidate[open_idx:i+1]
                        parsed = json.loads(sub)
                        return parsed, sub
                    except Exception:
                        continue

    # nothing works
    return None, text

# ---------------------------
# Strong JSON-only prompt builder
# ---------------------------
def build_json_prompt(prescription_text: str, detected_json: str, interactions_json: str, allergy_json: str) -> str:
    prompt = f"""
You are an AI medical prescription verification assistant.

IMPORTANT INSTRUCTIONS:
1) Return EXACTLY one valid JSON object and NOTHING else.
2) Do not include explanations or markdown.
3) Place the JSON only between these markers: <<<JSON>>> and <<<ENDJSON>>>.

Prescription text:
{prescription_text}

Detected drugs (JSON):
{detected_json}

Interactions:
{interactions_json}

Allergy warnings:
{allergy_json}

The JSON must include these keys: drug_validation (bool), safety_checks (bool), completeness_check (bool), notes (string).

Return exactly like:
<<<JSON>>>
{{
  "drug_validation": true,
  "safety_checks": true,
  "completeness_check": false,
  "notes": "Short summary or flagged issues here."
}}
<<<ENDJSON>>>
"""
    return prompt

# ---------------------------
# Load LLM pipeline safely (accelerate/device_map handling)
# ---------------------------
llm = None
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

    use_cuda = torch.cuda.is_available()

    # Choose dtype
    dtype = torch.float16 if use_cuda else torch.float32

    logging.info("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)

    logging.info("Loading model (this may take a minute)...")
    # Use device_map="auto" to let accelerate place shards
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=dtype,
        device_map="auto",
        trust_remote_code=True
    )

    # Important: do NOT pass device= when model used device_map="auto"
    logging.info("Creating pipeline...")
    llm = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=MAX_NEW_TOKENS,
        temperature=0.0,
        do_sample=False
    )
    logging.info("LLM pipeline created successfully.")
except Exception as e:
    logging.exception("LLM load failed; using mock LLM. Error: %s", e)
    llm = None

# ---------------------------
# LLM call wrapper (with mock fallback)
# ---------------------------
def call_llm(prompt: str) -> str:
    """
    Returns raw text output from model or a mock output when model not loaded.
    """
    if llm is not None:
        res = llm(prompt)
        # pipeline returns list with 'generated_text'
        raw = res[0].get("generated_text", "") if isinstance(res, list) else str(res)
        return raw
    else:
        # Mock output for testing when model not loaded
        logging.info("LLM not loaded; returning mock output (for testing).")
        mock = f"""
Debug: returning mock. <<<JSON>>>
{{
"drug_validation": true,
"safety_checks": false,
"completeness_check": false,
"notes": "Mock: paracetamol dose high and prescriber missing."
}}
<<<ENDJSON>>>
"""
        return mock

# ---------------------------
# Main verify function (for Gradio)
# ---------------------------
def verify(pres_image, pres_text, patient_info, allergies, current_meds):
    disclaimer = ("*Disclaimer:* This tool provides automated checks only. "
                  "It is not a substitute for a licensed clinician or pharmacist.")

    # 1) Get text (OCR fallback)
    if (not pres_text or str(pres_text).strip() == "") and pres_image is not None:
        try:
            pres_text = ocr_extract(pres_image)
        except Exception as e:
            logging.exception("OCR extraction error: %s", e)
            pres_text = ""

    if not pres_text or str(pres_text).strip() == "":
        return disclaimer + "\n\n‚ùå No prescription text found. Upload an image or paste text.", {}

    pres_text = str(pres_text).strip()

    # 2) Parse lines to detect drugs
    lines = [ln.strip() for ln in pres_text.splitlines() if ln.strip()]
    detected = []
    for line in lines:
        words = re.findall(r"[A-Za-z¬µ\w\-]+|", line)
        if not words:
            continue
        drug_info = None
        matched_token = None
        for w in words:
            info = match_drug(w)
            if info:
                drug_info = info
                matched_token = w
                break
        dose = extract_dose(line)
        if drug_info:
            if dose is None:
                dose_flag = "Dose not detected"
            else:
                min_ok = drug_info.get("min")
                max_ok = drug_info.get("max")
                if min_ok is None or max_ok is None:
                    dose_flag = "No reference range available"
                elif min_ok <= dose <= max_ok:
                    dose_flag = "Dose in safe range"
                else:
                    dose_flag = f"Dose OUTSIDE safe range ({min_ok}‚Äì{max_ok} mg)"
            detected.append({
                "raw_line": line,
                "matched_token": matched_token,
                "drug": drug_info["generic"],
                "dose_mg": dose,
                "dose_check": dose_flag
            })

    # 3) Interactions
    drug_names = sorted({d["drug"] for d in detected})
    interaction_msgs = []
    for i in range(len(drug_names)):
        for j in range(i+1, len(drug_names)):
            pair = tuple(sorted((drug_names[i], drug_names[j])))
            if pair in INTERACTIONS:
                interaction_msgs.append({"pair": pair, "message": INTERACTIONS[pair]})

    # 4) Allergy check (simple substring)
    allergy_warnings = []
    if allergies:
        allergy_list = [normalize_name(a) for a in re.split(r'[;,\n]+', str(allergies)) if a.strip()]
        for d in detected:
            for a in allergy_list:
                if a and (a in d["drug"] or a in d["raw_line"].lower()):
                    allergy_warnings.append(f"Allergy match: patient allergy '{a}' vs detected drug '{d['drug']}'")

    # 5) Build prompt and call LLM
    prompt = build_json_prompt(
        prescription_text=pres_text,
        detected_json=json.dumps(detected, indent=2),
        interactions_json=json.dumps(interaction_msgs, indent=2),
        allergy_json=json.dumps(allergy_warnings, indent=2)
    )

    raw = call_llm(prompt)
    logging.info("LLM raw output (preview):\n%s", raw[:1000])

    parsed, candidate_text = extract_json_from_text(raw)
    if parsed is None:
        # fallback: produce a deterministic summary
        fallback_summary_lines = [
            f"Detected drugs: {', '.join([d['drug'] for d in detected]) if detected else 'None'}",
            "Dose notes: " + ("; ".join([f'{d['drug']}: {d['dose_check']}' for d in detected]) if detected else "none"),
            "Interactions: " + (", ".join([i['message'] for i in interaction_msgs]) if interaction_msgs else "None detected"),
            "Allergy warnings: " + (", ".join(allergy_warnings) if allergy_warnings else "None detected"),
            "Missing info: Check patient identifiers, age/weight, prescriber name/license, dosing schedule/frequency."
        ]
        summary = disclaimer + "\n\n" + "Could not parse LLM JSON output reliably.\n\n" + "\n\n".join(fallback_summary_lines) + "\n\nRaw LLM preview:\n" + raw[:2000]
        structured = {
            "error": "parse_failed",
            "raw_output_preview": raw[:2000],
            "detected": detected,
            "interactions": interaction_msgs,
            "allergy_warnings": allergy_warnings
        }
        return summary, structured
    else:
        # success
        notes = parsed.get("notes", "")
        summary = disclaimer + "\n\n==== AI PRESCRIPTION VALIDATION ====\n\n" + notes
        structured = {
            "patient_info": patient_info,
            "allergies": allergies,
            "current_meds_input": current_meds,
            "detected": detected,
            "interactions": interaction_msgs,
            "allergy_warnings": allergy_warnings,
            "llm_parsed": parsed
        }
        return summary, structured

# ---------------------------
# Gradio UI
# ---------------------------
def build_and_launch_ui():
    with gr.Blocks() as demo:
        gr.Markdown("## üß† AI Medical Prescription Verification ‚Äî Granite 3.3 2B (Colab-ready)")
        with gr.Row():
            img_in = gr.Image(label="Upload Prescription Image (optional)")
            txt_in = gr.Textbox(label="Paste Prescription Text (optional)", lines=8)
        patient = gr.Textbox(label="Patient Info (optional)")
        allergy = gr.Textbox(label="Allergies (comma-separated)")
        meds = gr.Textbox(label="Current Medications (comma-separated)")
        btn = gr.Button("VERIFY")
        out_report = gr.Textbox(label="Verification Report", lines=20)
        out_json = gr.JSON(label="Structured Output")
        btn.click(verify, inputs=[img_in, txt_in, patient, allergy, meds], outputs=[out_report, out_json])
        demo.launch(share=True)

# If run as a script/cell, launch UI
if __name__ == "__main__":
    build_and_launch_ui()